{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from utils import heston_pricer\n",
    "from utils import rBergomi_pricer\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureScaler(object):\n",
    "    def __init__(self, train):\n",
    "        self.mean = train.mean(axis=0)\n",
    "        self.std = train.std(axis=0)\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return (df - self.mean) / self.std\n",
    "    \n",
    "    def inverse_transform(self, df):\n",
    "        return df * self.std + self.mean\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.mean, self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn(input_length, hidden_layer_sizes = [20, 10, 5]):\n",
    "    model = K.models.Sequential()\n",
    "    kernel_init = K.initializers.RandomNormal(stddev=np.sqrt(2.0/input_length))\n",
    "    model.add(K.layers.Dense(units=hidden_layer_sizes[0], activation='relu', kernel_initializer=kernel_init, \n",
    "                             input_shape=[input_length]))\n",
    "    for layer_idx in range(1, len(hidden_layer_sizes)):\n",
    "        kernel_init = K.initializers.RandomNormal(stddev=np.sqrt(2.0/hidden_layer_sizes[layer_idx-1]))\n",
    "        model.add(K.layers.Dense(units=hidden_layer_sizes[layer_idx], kernel_initializer=kernel_init, activation='relu'))\n",
    "    kernel_init = K.initializers.RandomNormal(stddev=np.sqrt(2.0/hidden_layer_sizes[-1]))\n",
    "    model.add(K.layers.Dense(units=1, kernel_initializer=kernel_init, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X_train, Y_train, X_val, Y_val, batch_size=128, n_epochs=50, hidden_layer_sizes=[20, 10, 5], verbose=1):\n",
    "    n_features = X_train.shape[1]\n",
    "    model = build_nn(n_features, hidden_layer_sizes)\n",
    "    \n",
    "    if verbose != 0:\n",
    "        print(\"Neural Network Architechture:\")\n",
    "        print(model.summary())\n",
    "        print('\\n')\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=verbose, mode='auto', restore_best_weights=True)\n",
    "    train_log = model.fit(X_train, Y_train, batch_size=batch_size, epochs=n_epochs, verbose=verbose, \n",
    "                          validation_data=(X_val, Y_val), callbacks=[early_stopping])\n",
    "    \n",
    "    return model, train_log.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_history(history):\n",
    "    plt.figure()\n",
    "    plt.plot(history['loss'], color='b', marker='o', label='train loss')\n",
    "    plt.plot(history['val_loss'], color='r', marker='o', label='val loss')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heston Model Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load generated synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/heston/ftse_0118_train.csv\").dropna()\n",
    "val = pd.read_csv(\"./data/heston/ftse_0118_val.csv\").dropna()\n",
    "test = pd.read_csv(\"./data/heston/ftse_0118_test.csv\").dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split input features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_origin, Y_train = train.iloc[:, :-1], train.iloc[:, [-1]]\n",
    "X_val_origin, Y_val = val.iloc[:, :-1], val.iloc[:, [-1]]\n",
    "X_test_origin, Y_test = test.iloc[:, :-1], test.iloc[:, [-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALER = FeatureScaler(X_train_origin)\n",
    "X_train = SCALER.transform(X_train_origin)\n",
    "X_val = SCALER.transform(X_val_origin)\n",
    "X_test = SCALER.transform(X_test_origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "hidden_layer_sizes = [20, 10, 5]\n",
    "batch_size = 128\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Neural Network, and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, history = train_nn(X_train, Y_train, X_val, Y_val, batch_size, n_epochs, hidden_layer_sizes, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot learning history curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model to local file for further use, or load a previously trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# save model\n",
    "\n",
    "model.save(\"heston.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the quantiles of the relative error on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(X_test).flatten()\n",
    "true_test = Y_test.values.flatten()\n",
    "relative_error = 100 * np.abs(pred_test - true_test) / true_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_re = sorted(relative_error)\n",
    "q_90 = sorted_re[int(0.90*len(relative_error))]\n",
    "q_95 = sorted_re[int(0.95*len(relative_error))]\n",
    "q_99 = sorted_re[int(0.99*len(relative_error))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.hist(relative_error, bins=np.linspace(0, 15, 151), density=True, rwidth=0.5)\n",
    "plt.xlim((0, 15))\n",
    "sns.despine(left=True, bottom=True, right=True)\n",
    "plt.grid(True)\n",
    "plt.axvline(x=q_90, ls='--', color='orange', label=r\"$q_{0.90}$\"+\"={:.2f}\".format(q_90))\n",
    "plt.axvline(x=q_95, ls='-.', color='purple', label=r\"$q_{0.95}$\"+\"={:.2f}\".format(q_95))\n",
    "plt.axvline(x=q_99, ls=':', color='green', label=r\"$q_{0.99}$\"+\"={:.2f}\".format(q_99))\n",
    "plt.legend(loc=1)\n",
    "plt.xlabel(\"relative error\")\n",
    "plt.ylabel(\"frequency density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a fixed parameter to generate the IV surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_moneyness = np.linspace(-0.1, 0.1, 21)\n",
    "maturity = np.linspace(0.01, 0.18, 18)\n",
    "log_moneyness, maturity = np.meshgrid(log_moneyness, maturity)\n",
    "\n",
    "# columns ['Moneyness', 'Time to Maturity (years)', 'lambda', 'vbar', 'eta', 'rho', 'v0']\n",
    "df = pd.DataFrame(columns=train.columns)\n",
    "df['Moneyness'] = np.exp(log_moneyness.flatten())\n",
    "df['Time to Maturity (years)'] = maturity.flatten()\n",
    "df['eta'] = 0.3877\n",
    "df['rho'] = -0.7165\n",
    "df['lambda'] = 1.3253\n",
    "df['v0'] = 0.0354\n",
    "df['vbar'] = 0.0174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['iv'] = df.progress_apply(lambda row: heston_pricer(row['lambda'], row['vbar'], row['eta'], row['rho'], row['v0'], \n",
    "                                                       0, 0, row['Time to Maturity (years)'], 1.0, row['Moneyness'])[1], \n",
    "                             axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = SCALER.transform(df.iloc[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['iv_nn'] = model.predict(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Log Moneyness'] = log_moneyness.flatten()\n",
    "df['re'] = np.abs(df['iv_nn'] - df['iv']) / df['iv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(data, values='re'):\n",
    "    \"\"\" Plots the heatmap of the `values` column w.r.t. `Time to Maturity` and `Log Moneyness`.\n",
    "    \"\"\"\n",
    "    data_sort = data.sort_values(values, ascending=True)\n",
    "    data_pivot = data_sort.pivot(index='Time to Maturity (years)', columns='Log Moneyness', values=values)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.heatmap(data_pivot, cmap=plt.cm.Spectral, cbar=True, \n",
    "                     xticklabels=data_pivot.columns.values.round(2), \n",
    "                     yticklabels=data_pivot.index.values.round(2))\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the heatmap of relative errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(df, 're')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot IV surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_iv_surface(data, x=\"Log Moneyness\", y='Time to Maturity (years)', z='iv'):\n",
    "    \"\"\" Plots the IV surface\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.azim = 120\n",
    "    ax.elev = 13\n",
    "    \n",
    "    ax.set_xlabel(x)\n",
    "    ax.set_ylabel(y)\n",
    "    ax.set_zlabel(z)\n",
    "\n",
    "    ax.invert_xaxis()\n",
    "    ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%0.2f'))\n",
    "    \n",
    "    surf = ax.plot_trisurf(data[x], data[y], data[z], antialiased=True, cmap = plt.cm.Spectral)\n",
    "    fig.colorbar(surf, shrink=0.7, aspect=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, the \"true\" IV surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iv_surface(df, z='iv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, the \"predicted\" IV surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_iv_surface(df, z='iv_nn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rBergomi Model Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load generated synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/rBergomi/ftse_0118_train.csv\").dropna()\n",
    "val = pd.read_csv(\"./data/rBergomi/ftse_0118_val.csv\").dropna()\n",
    "test = pd.read_csv(\"./data/rBergomi/ftse_0118_test.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns=['label','index'])\n",
    "val = val.drop(columns=['label','index'])\n",
    "test = test.drop(columns=['label','index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['H', 'Moneyness', 'Time to Maturity (years)', 'eta', 'rho', 'v0', 'iv']]\n",
    "val = val[['H', 'Moneyness', 'Time to Maturity (years)', 'eta', 'rho', 'v0', 'iv']]\n",
    "test = test[['H', 'Moneyness', 'Time to Maturity (years)', 'eta', 'rho', 'v0', 'iv']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split input features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_origin, Y_train = train.iloc[:, :-1], train.iloc[:, [-1]]\n",
    "X_val_origin, Y_val = val.iloc[:, :-1], val.iloc[:, [-1]]\n",
    "X_test_origin, Y_test = test.iloc[:, :-1], test.iloc[:, [-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALER = FeatureScaler(X_train_origin)\n",
    "X_train = SCALER.transform(X_train_origin)\n",
    "X_val = SCALER.transform(X_val_origin)\n",
    "X_test = SCALER.transform(X_test_origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "hidden_layer_sizes = [20, 10, 5]\n",
    "batch_size = 128\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Neural Network and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = train_nn(X_train, Y_train, X_val, Y_val, batch_size, n_epochs, hidden_layer_sizes, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot learning history curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model to local file for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"rBergomi.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the quantiles of the relative error on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(X_test).flatten()\n",
    "true_test = Y_test.values.flatten()\n",
    "relative_error = 100 * np.abs(pred_test - true_test) / true_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_re = sorted(relative_error)\n",
    "q_90 = sorted_re[int(0.90*len(relative_error))]\n",
    "q_95 = sorted_re[int(0.95*len(relative_error))]\n",
    "q_99 = sorted_re[int(0.99*len(relative_error))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.hist(relative_error, bins=np.linspace(0, 15, 151), density=True, rwidth=0.5)\n",
    "plt.xlim((0, 15))\n",
    "sns.despine(left=True, bottom=True, right=True)\n",
    "plt.grid(True)\n",
    "plt.axvline(x=q_90, ls='--', color='orange', label=r\"$q_{0.90}$\"+\"={:.2f}\".format(q_90))\n",
    "plt.axvline(x=q_95, ls='-.', color='purple', label=r\"$q_{0.95}$\"+\"={:.2f}\".format(q_95))\n",
    "plt.axvline(x=q_99, ls=':', color='green', label=r\"$q_{0.99}$\"+\"={:.2f}\".format(q_99))\n",
    "plt.legend(loc=1)\n",
    "plt.xlabel(\"relative error\")\n",
    "plt.ylabel(\"frequency density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a fixed parameter to generate the IV surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_moneyness = np.linspace(-0.1, 0.1, 21)\n",
    "maturity = np.linspace(0.01, 0.18, 18)\n",
    "log_moneyness, maturity = np.meshgrid(log_moneyness, maturity)\n",
    "\n",
    "# columns ['Moneyness', 'Time to Maturity (years)', 'H', 'eta', 'rho', 'v0']\n",
    "df = pd.DataFrame(columns=train.columns)\n",
    "df['Moneyness'] = np.exp(log_moneyness.flatten())\n",
    "df['Time to Maturity (years)'] = maturity.flatten()\n",
    "df['H'] = 0.07\n",
    "df['eta'] = 1.9\n",
    "df['rho'] = -0.9\n",
    "df['v0'] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['H', 'Moneyness', 'Time to Maturity (years)', 'eta', 'rho', 'v0', 'iv']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['iv'] = df.progress_apply(lambda row: rBergomi_pricer(row['H'], row['eta'], row['rho'], row['v0'], \n",
    "                                                         row['Time to Maturity (years)'], row['Moneyness'], 1.0)[1], \n",
    "                             axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = SCALER.transform(df.iloc[:, :-1])\n",
    "df['iv_nn'] = model.predict(scaled_features)\n",
    "df['Log Moneyness'] = log_moneyness.flatten()\n",
    "df['re'] = np.abs(df['iv_nn'] - df['iv']) / df['iv']\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(df, 're')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iv_surface(df, z='iv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iv_surface(df, z='iv_nn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part implements the Deep calibration algorithm (LM combined with NN regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import heston_pricer, rBergomi_pricer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a model parameters initializer, which will be used to initialize the model parameters before the LM calibrating loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import truncnorm\n",
    "\n",
    "def model_parameters_initializer(model='heston', random_seed=None):\n",
    "    \"\"\" Initialize model parameters\n",
    "    \"\"\"\n",
    "    if model == 'heston':\n",
    "        params = [\n",
    "            10 * np.random.rand(), # lambda\n",
    "            np.random.rand(), # vbar\n",
    "            5 * np.random.rand(), # eta\n",
    "            -1 * np.random.rand(), # rho\n",
    "            np.random.rand() #v0\n",
    "        ]\n",
    "        names = ['lambda', 'vbar', 'eta', 'rho', 'v0']\n",
    "    elif model == 'rbergomi' or model == 'rBergomi':\n",
    "        params = [\n",
    "            truncnorm.rvs(-1.2, 8.6, 0.07, 0.05), # H\n",
    "            truncnorm.rvs(-3, 3, 2.5, 0.5), # eta\n",
    "            truncnorm.rvs(-0.25, 2.25, -0.95, 0.2), # rho\n",
    "            truncnorm.rvs(-2.5, 7, 0.3, 0.1) # v0\n",
    "        ]\n",
    "        names = ['H', 'eta', 'rho', 'v0']\n",
    "    else:\n",
    "        raise NameError(\"No such model name: {}\".format(model))\n",
    "    return params, names\n",
    "params_, names_ = model_parameters_initializer(model='rBergomi', random_seed=None)\n",
    "params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to return the prediction and the Jacobian matrix of a neural network w.r.t. a specific input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label_jac(sess, model, test_inputs,model_name):\n",
    "    \"\"\" Use a trained model to predict and to return jacobian matrix\n",
    "    \"\"\"\n",
    "    if model_name=='heston':\n",
    "        model = keras.models.load_model('heston.h5')\n",
    "    else:\n",
    "        model = keras.models.load_model('rBergomi.h5')\n",
    "    print(\"Loaded Model from disk\")\n",
    "    #compile and evaluate loaded model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    pred = model.predict(test_inputs)\n",
    "    grad_func = tf.gradients(model.output, model.input)\n",
    "    jac = sess.run(grad_func, feed_dict={model.input: test_inputs})[0]\n",
    "    return pred, jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_calibration(tf_sess, nn, K_T, market_quotes, model_name='heston', lambd_init=0.1, beta0=0.25, beta1=0.75, max_iter=1000, tol=1e-8):\n",
    "    \"\"\" Combines LM algorithm with a NN regressor to calibrating model parameters.\n",
    "    \"\"\"\n",
    "    # initialize model parameters\n",
    "    params, param_names = model_parameters_initializer(model_name)\n",
    "    \n",
    "    # initalize learning step\n",
    "    lambd = lambd_init\n",
    "    \n",
    "    n_samples = K_T.shape[0]\n",
    "    n_params = len(params)\n",
    "    I = np.eye(n_params)\n",
    "    Q = market_quotes.reshape((-1, 1)) # shape: [n_samples, 1]\n",
    "    K_T_values = K_T.values\n",
    "    \n",
    "    iter_count = 0\n",
    "    \n",
    "    # history to store some useful information during training \n",
    "    history = {\n",
    "        'delta_params': {k: [] for k in param_names},\n",
    "        'R': [],\n",
    "        'lambda': [],\n",
    "        'c_mu': []\n",
    "    }\n",
    "    \n",
    "    # build a input dataframe by combining K_T and model parameters\n",
    "    for i in range(len(param_names)):\n",
    "        K_T[param_names[i]] = params[i]\n",
    "    if model_name=='heston':\n",
    "\n",
    "        input_data = K_T[['Moneyness','Time to Maturity (years)','lambda','vbar','eta','rho','v0']].values      ##np.insert(K_T_values, [2]*n_params, params, axis=1) # shape: [n_samples, n_params+2]\n",
    "    else:\n",
    "        input_data = K_T[['H', 'Moneyness', 'Time to Maturity (years)', 'eta', 'rho', 'v0']].values\n",
    "    \n",
    "    iv_nn, J = predict_label_jac(tf_sess, nn, input_data,model_name)\n",
    "    R = iv_nn - Q # shape: [n_samples, 1]\n",
    "    J = J[:, 2:] # shape: [n_samples, n_params], excluding K and T\n",
    "    delta_params = np.linalg.pinv(J.T.dot(J) + lambd * I).dot(J.T.dot(R)).flatten() # vector size: [n_params,]\n",
    "    \n",
    "    history['R'].append(np.linalg.norm(R))\n",
    "    history['lambda'].append(lambd)\n",
    "    for param_idx, param_name in enumerate(param_names):\n",
    "        history['delta_params'][param_name].append(delta_params[param_idx])\n",
    "\n",
    "    while iter_count < max_iter and np.linalg.norm(delta_params) > tol:\n",
    "        if iter_count % 50 == 0:\n",
    "            logging.info(\"{}/{} iteration\".format(iter_count+1, max_iter))\n",
    "        params_new = params - delta_params\n",
    "        input_data_new = np.insert(K_T_values, [2]*n_params, params_new, axis=1)\n",
    "        iv_nn_new, J_new = predict_label_jac(tf_sess, nn, input_data_new,model_name)\n",
    "        R_new = iv_nn_new - Q\n",
    "        J_new = J_new[:, 2:]\n",
    "        R_norm = np.linalg.norm(R)\n",
    "        c_mu = (R_norm - np.linalg.norm(R_new)) / (R_norm - np.linalg.norm(R - J.dot(delta_params)))\n",
    "        \n",
    "        history['c_mu'].append(c_mu)\n",
    "        \n",
    "        if c_mu <= beta0:\n",
    "            # reject delta_params\n",
    "            lambd *= 2 # too slow, use greater lambd\n",
    "        else:\n",
    "            params = params_new\n",
    "            R = R_new\n",
    "            J = J_new\n",
    "        if c_mu >=beta1:\n",
    "            lambd /= 2.0\n",
    "        \n",
    "        delta_params = np.linalg.pinv(J.T.dot(J) + lambd * I).dot(J.T.dot(R)).flatten() # vector size: [n_params, ]\n",
    "        iter_count += 1\n",
    "        \n",
    "        history['R'].append(np.linalg.norm(R))\n",
    "        history['lambda'].append(lambd)\n",
    "        for param_idx, param_name in enumerate(param_names):\n",
    "            history['delta_params'][param_name].append(delta_params[param_idx])\n",
    "    if iter_count < max_iter:\n",
    "        logging.info(\"Leave iterations after {} iters\".format(iter_count))\n",
    "        \n",
    "    return dict(zip(param_names, params)), history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_T = X_test.iloc[:500,:][['Moneyness','Time to Maturity (years)']]\n",
    "K_T_origin = X_test_origin.iloc[:500, :][['Moneyness','Time to Maturity (years)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market parameters\n",
    "S0 = 1\n",
    "r = 0 \n",
    "\n",
    "# Heston parameters \n",
    "lambd = 1.3253 \n",
    "vbar = 0.0354 \n",
    "eta = 0.3877 \n",
    "rho = -0.7165 \n",
    "v0 = 0.0174 \n",
    "q = 0\n",
    "\n",
    "market_quotes = np.array([heston_pricer(lambd, vbar, eta, rho, v0, r, q, K_T_origin.iloc[i, 1], S0, K_T_origin.iloc[i, 0])[1] for i in range(K_T.shape[0])])\n",
    "market_quotes = market_quotes.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.progress_apply(lambda row: rBergomi_pricer(row['H'], row['eta'], row['rho'], row['v0'], \n",
    "#                                                          row['Time to Maturity (years)'], row['Moneyness'], 1.0)[1], \n",
    "#                              axis=1)\n",
    "\n",
    "S0 = 1                            \n",
    "eta = 1.9\n",
    "rho = -0.9\n",
    "H = 0.07\n",
    "v0 = 0.01\n",
    "market_quotes = np.array([rBergomi_pricer(H, eta, rho, v0, K_T_origin.iloc[i, 1], K_T_origin.iloc[i, 0], S0)[1] for i in range(K_T.shape[0])])\n",
    "market_quotes = market_quotes.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx = ~np.isnan(market_quotes.flatten())\n",
    "K_T_input = K_T.iloc[valid_idx, :]\n",
    "market_quotes = market_quotes[valid_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras as K\n",
    "import keras\n",
    "params, history = deep_calibration(sess, model, K_T_input, market_quotes, model_name='rbergomi', lambd_init=0.01, beta0=0.25, beta1=0.75, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_hat = pd.DataFrame([params.values()], columns=params.keys())\n",
    "mu_hat['Moneyness'] = 0\n",
    "mu_hat['Time to Maturity (years)'] = 0\n",
    "SCALER.inverse_transform(mu_hat[['H', 'Moneyness', 'Time to Maturity (years)', 'eta', 'rho', 'v0']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureScaler(object):\n",
    "    def __init__(self, train):\n",
    "        self.mean = train.mean(axis=0)\n",
    "        self.std = train.std(axis=0)\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return (df - self.mean) / self.std\n",
    "    \n",
    "    def inverse_transform(self, df):\n",
    "        return df * self.std + self.mean\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.mean, self.std\n",
    "\n",
    "SCALER.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 24 2022, 18:44:54) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "0b9be10dbf2f757c6dd8c30fd0ea7dd38ee9e768dd20fb7638a80f027924761c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
